# -*- coding: utf-8 -*-
"""ML Assignment 2-Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j5J6wxRgWsk9Z3fHb1PiAp4MERytReSE
"""

! pip install streamlit

import os
import sys
import joblib
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm

# sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, roc_auc_score, precision_score,
    recall_score, f1_score, matthews_corrcoef
)

# Models
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Constants
MODEL_DIR = 'model'
OUTPUT_METRICS = 'telecom_churn_performance.csv'
CORR_IMAGE = 'feature_heatmap.png'
IMG_PATH_BALANCE = 'Visual.png'
IMG_PATH_CORR = 'Numeric_values_ correlation.png'
IMG_PATH_CONTRACT = 'Categorical_values_correlation.png'

# ---------------------------------------------------------
# 1. LOAD DATA FROM DRIVE LINK
# ---------------------------------------------------------
# We use 'gdown' to download the file directly using its ID
# This is faster than mounting drive if you just need one file.
# file_id = '1NGSDFY-_IWr1IxkdO5bEMg46p551Coi6'
# download_url = f'https://drive.google.com/uc?id={file_id}'
# output_file = 'telco_churn.csv'
output_file = '/content/drive/MyDrive/ML Assignment2/telco-customer-churn.csv'

print(">>> Downloading dataset...")
try:
    # Check if file already exists to avoid re-downloading
    if not os.path.exists(output_file):
        import gdown
        gdown.download(download_url, output_file, quiet=False)

    df = pd.read_csv(output_file)
    print("âœ… Data Loaded Successfully")

except Exception as e:
    print(f"âŒ Error loading data: {e}")
    # Fallback: If gdown fails, ask user to upload manually
    print("   Tip: If this fails, upload 'telco_churn.csv' to the Files tab on the left.")

# ---------------------------------------------------------
# 2. CLEANING (Specific to Telco Churn)
# ---------------------------------------------------------
# A. Fix TotalCharges (Convert space ' ' to NaN, then 0)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)

# B. Drop CustomerID (Not useful for training)
if 'customerID' in df.columns:
    df = df.drop('customerID', axis=1)

# C. Prepare X and y
target_col = 'Churn'
X = df.drop(columns=[target_col])
y = df[target_col].map({'Yes': 1, 'No': 0})  # Convert Yes/No to 1/0

print(f"   Features shape: {X.shape}")
print(f"   Target shape:   {y.shape}")

df.head()

"""Target Balance (Crucial for Churn)"""

import seaborn as sns
import matplotlib.pyplot as plt

# ---------------------------------------------------------
# 2. EXPLORATORY ANALYSIS (Target Balance)
# ---------------------------------------------------------
print("\n>>> Generating Target Distribution Plot...")
IMG_PATH_BALANCE = "churn_balance.png"

plt.figure(figsize=(6, 4))

# Countplot automatically counts the unique values
ax = sns.countplot(x='Churn', data=df, palette='viridis')

plt.title("Class Distribution (Churn vs Non-Churn)")
plt.xlabel("Churn Status")
plt.ylabel("Number of Customers")

# Add count labels on top of bars
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + 0.3, p.get_height() + 50))

plt.tight_layout()
plt.savefig(IMG_PATH_BALANCE, dpi=300)
print(f"    Saved balance plot to '{IMG_PATH_BALANCE}'")

"""Numerical Correlation Heatmap"""

# ---------------------------------------------------------
# 3. NUMERICAL CORRELATION HEATMAP
# ---------------------------------------------------------
print("\n>>> Generating Numerical Correlation Heatmap...")
IMG_PATH_CORR = "numeric_correlation.png"

plt.figure(figsize=(8, 6))

# Select only numeric features
numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']
corr_matrix = df[numeric_features].corr()

sns.heatmap(
    corr_matrix,
    annot=True,
    fmt=".2f",
    cmap='coolwarm',
    linewidths=0.5
)

plt.title("Correlation: Tenure vs Charges")
plt.tight_layout()
plt.savefig(IMG_PATH_CORR, dpi=300)
print(f"    Saved heatmap to '{IMG_PATH_CORR}'")

"""Categorical Insight (Churn by Contract)"""

# ---------------------------------------------------------
# 4. CATEGORICAL INSIGHT (Churn by Contract)
# ---------------------------------------------------------
print("\n>>> Generating Churn by Contract Plot...")
IMG_PATH_CONTRACT = "churn_by_contract.png"

plt.figure(figsize=(10, 6))

# hue='Churn' splits the bars into Yes/No side-by-side
sns.countplot(x='Contract', hue='Churn', data=df, palette='Paired')

plt.title("Churn Rate by Contract Type")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig(IMG_PATH_CONTRACT, dpi=300)
print(f"    Saved contract plot to '{IMG_PATH_CONTRACT}'")

# ---------------------------------------------------------
# 3. SPLIT & SAVE X_TEST + Y_TEST
# ---------------------------------------------------------
# Split: 80% Train, 20% Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# RESET INDEX (Crucial step discussed earlier)
# If we don't do this, the rows won't align correctly when we join them
X_test_reset = X_test.reset_index(drop=True)
y_test_reset = y_test.reset_index(drop=True)

# Combine into one DataFrame
test_combined = pd.concat([X_test_reset, y_test_reset], axis=1)

# Save to CSV
test_combined.to_csv('test_data_combined.csv', index=False)

print("\nâœ… Success! 'test_data_combined.csv' has been saved.")
print("   We can download it from the Files tab on the left.")
print("   This file is ready for Git repo or Streamlit app.")

"""Fit Encoder on TRAIN only- We use fit_transform on training data, but only transform on test data.

"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# 1. Define your columns
# Numeric columns need Scaling
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
# Categorical columns need One-Hot Encoding
cat_cols = [c for c in X_train.columns if c not in num_cols]

# 2. Create the Transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)
    ],
    verbose_feature_names_out=False # Keeps names clean (e.g. "Contract_Two year")
)

# 3. Fit on TRAIN, Transform TEST
# We fit only on training data to avoid data leakage
X_train_encoded = preprocessor.fit_transform(X_train)
X_test_encoded = preprocessor.transform(X_test)

# 4. Get the NEW column names (Crucial step!)
new_feature_names = preprocessor.get_feature_names_out()

print(f"Original feature count: {X_train.shape[1]}")
print(f"New feature count (after encoding): {len(new_feature_names)}")

import joblib

# Save the preprocessing pipeline so the app can use it
joblib.dump(preprocessor, 'preprocessor.pkl')
print("âœ… Preprocessor saved as 'preprocessor.pkl'")

import os
import sys
import pandas as pd
import joblib
from tqdm import tqdm

# 1. Import Classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# 2. Import Metrics
from sklearn.metrics import (
    accuracy_score, roc_auc_score, precision_score,
    recall_score, f1_score, matthews_corrcoef
)

# ---------------------------------------------------------
# CONFIGURATION
# ---------------------------------------------------------
MODEL_DIR = "saved_models"
OUTPUT_METRICS = "model_performance.csv"

# ---------------------------------------------------------
# 4. MODELING UTILITIES
# ---------------------------------------------------------

def manage_model_training(algo_name, file_name, clf, x_data, y_data):
    """
    Checks for existing model file; trains and saves if missing.
    """
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)

    full_path = os.path.join(MODEL_DIR, file_name)

    # If model exists, load it to save time
    if os.path.exists(full_path):
        return joblib.load(full_path), "LOADED from disk"
    else:
        # Train and Save
        clf.fit(x_data, y_data)
        joblib.dump(clf, full_path)
        return clf, "TRAINED and saved"

def calculate_performance(algo_name, model, x_val, y_val):
    """
    Generates a dictionary of performance metrics.
    Optimized for BINARY CLASSIFICATION (Churn = 1).
    """
    preds = model.predict(x_val)

    # Calculate Probabilities for AUC
    try:
        if hasattr(model, "predict_proba"):
            # For Binary, we take the probability of the Positive Class (1)
            probs = model.predict_proba(x_val)[:, 1]
            auc_val = roc_auc_score(y_val, probs)
        else:
            auc_val = 0.0
    except:
        auc_val = 0.0

    return {
        "Algorithm": algo_name,
        "Accuracy": accuracy_score(y_val, preds),
        "AUC": auc_val,
        # 'binary' focus on the "Churn" class (1)
        "Precision": precision_score(y_val, preds, zero_division=0),
        "Recall": recall_score(y_val, preds, zero_division=0),
        "F1-Score": f1_score(y_val, preds, zero_division=0),
        "MCC": matthews_corrcoef(y_val, preds)
    }

# ---------------------------------------------------------
# 5. EXECUTION PIPELINE
# ---------------------------------------------------------

# Dictionary of classifiers
classifiers = [
    ("Logistic Regression", "log_reg.pkl", LogisticRegression(max_iter=1500, random_state=42)),
    ("Decision Tree", "dt_clf.pkl", DecisionTreeClassifier(random_state=42)),
    ("K-Nearest Neighbors", "knn.pkl", KNeighborsClassifier(n_neighbors=5)),
    ("Gaussian NB", "gnb.pkl", GaussianNB()),
    ("Random Forest", "rf_clf.pkl", RandomForestClassifier(n_estimators=100, random_state=42)),
    ("XGBoost", "xgb.pkl", XGBClassifier(eval_metric='logloss', random_state=42))
]

performance_log = []

print("\n>>> Starting Model Evaluation Loop...")

# Progress Bar
with tqdm(classifiers, desc="Initializing Models", unit="model", leave=True, file=sys.stdout) as pbar:
    for name, f_name, obj in pbar:
        pbar.set_description(f"Processing {name}")

        # 1. Train or Load (Using YOUR variables: X_train_encoded, y_train)
        trained_model, status = manage_model_training(name, f_name, obj, X_train_encoded, y_train)

        # Write status to console
        pbar.write(f"   > {name}: {status}")

        # 2. Calculate Metrics (Using YOUR variables: X_test_encoded, y_test)
        metrics = calculate_performance(name, trained_model, X_test_encoded, y_test)
        performance_log.append(metrics)

# ---------------------------------------------------------
# 6. EXPORT RESULTS
# ---------------------------------------------------------
final_df = pd.DataFrame(performance_log)

# Sort by F1-Score (Better for Churn than Accuracy)
final_df = final_df.sort_values(by="F1-Score", ascending=False)

# Reorder columns for readability
col_order = ["Algorithm", "Accuracy", "AUC", "Precision", "Recall", "F1-Score", "MCC"]
final_df = final_df[col_order]

final_df.to_csv(OUTPUT_METRICS, index=False)

print(f"\n{'-'*60}")
print(f"Pipeline Complete. Metrics saved to '{OUTPUT_METRICS}'")
print(f"{'-'*60}")
print(final_df)

"""Streamlit code"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import joblib
# import os
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.metrics import (
#     accuracy_score, roc_auc_score, precision_score,
#     recall_score, f1_score, matthews_corrcoef, confusion_matrix
# )
# 
# # ---------------------------------------------------------
# # 1. CONFIGURATION
# # ---------------------------------------------------------
# st.set_page_config(page_title="Telco Churn AI", layout="wide", page_icon="ðŸ“¡")
# 
# st.markdown("""
#     <style>
#     .main { background-color: #F8F9FA; font-family: 'Segoe UI', sans-serif; }
#     .stButton>button {
#         width: 100%; border-radius: 6px; height: 3em;
#         background-color: #007BFF; color: white; font-weight: 600;
#     }
#     .stButton>button:hover { background-color: #0056b3; }
#     </style>
#     """, unsafe_allow_html=True)
# 
# # ---------------------------------------------------------
# # 2. GLOBAL CONSTANTS
# # ---------------------------------------------------------
# TARGET_CLASSES = ['No Churn', 'Churn']
# 
# if 'test_data' not in st.session_state:
#     st.session_state['test_data'] = None
# if 'test_targets' not in st.session_state:
#     st.session_state['test_targets'] = None
# 
# # ---------------------------------------------------------
# # 3. UTILITY FUNCTIONS
# # ---------------------------------------------------------
# @st.cache_resource
# def load_assets():
#     try:
#         preprocessor = joblib.load('preprocessor.pkl')
#         return preprocessor
#     except FileNotFoundError:
#         st.error("âš ï¸ 'preprocessor.pkl' not found. Please run the training notebook first.")
#         return None
# 
# def get_trained_model(selection):
#     model_map = {
#         "Logistic Regression": "log_reg.pkl",
#         "Decision Tree": "dt_clf.pkl",
#         "K-Nearest Neighbors": "knn.pkl",
#         "Naive Bayes (Gaussian)": "gnb.pkl",
#         "Random Forest": "rf_clf.pkl",
#         "XGBoost": "xgb.pkl"
#     }
#     f_path = os.path.join('saved_models', model_map.get(selection, ""))
#     if os.path.exists(f_path):
#         return joblib.load(f_path)
#     return None
# 
# def compute_metrics(clf, x_data, y_data):
#     # Ensure y_data contains no NaNs before calculating
#     if np.isnan(y_data).any():
#         return None
# 
#     preds = clf.predict(x_data)
#     auc_score = 0.0
#     try:
#         if hasattr(clf, "predict_proba"):
#             probs = clf.predict_proba(x_data)[:, 1]
#             auc_score = roc_auc_score(y_data, probs)
#     except:
#         pass
# 
#     return {
#         "Accuracy": accuracy_score(y_data, preds),
#         "AUC": auc_score,
#         "Precision": precision_score(y_data, preds, zero_division=0),
#         "Recall": recall_score(y_data, preds, zero_division=0),
#         "F1": f1_score(y_data, preds, zero_division=0),
#         "MCC": matthews_corrcoef(y_data, preds),
#         "predictions": preds
#     }
# 
# # ---------------------------------------------------------
# # 4. SIDEBAR SETUP
# # ---------------------------------------------------------
# st.sidebar.title("ðŸ“¡ Menu")
# app_mode = st.sidebar.radio("Navigate:", ["Batch Prediction Tool", "Model Insights"])
# 
# if app_mode == "Batch Prediction Tool":
#     st.sidebar.markdown("---")
#     st.sidebar.subheader("Settings")
#     selected_algorithm = st.sidebar.selectbox(
#         "Choose Algorithm",
#         ("Logistic Regression", "Decision Tree", "K-Nearest Neighbors",
#          "Naive Bayes (Gaussian)", "Random Forest", "XGBoost")
#     )
# 
# # ---------------------------------------------------------
# # 5. PAGE: PREDICTION TOOL
# # ---------------------------------------------------------
# if app_mode == "Batch Prediction Tool":
#     st.title("ðŸ“¡ Telco Customer Churn Prediction")
#     st.markdown("Upload a CSV of customers to identify who is at risk of leaving.")
#     st.markdown("---")
# 
#     left_col, right_col = st.columns([1, 2])
# 
#     with left_col:
#         st.subheader("1. Upload Data")
#         st.info("Upload a CSV file. It must contain the standard Telco columns.")
# 
#         data_file = st.file_uploader("Drop CSV Here", type=["csv"])
# 
#         if data_file:
#             try:
#                 # Read CSV
#                 raw_df = pd.read_csv(data_file)
# 
#                 # Cleanup: Force TotalCharges to numeric
#                 if 'TotalCharges' in raw_df.columns:
#                     raw_df['TotalCharges'] = pd.to_numeric(raw_df['TotalCharges'], errors='coerce').fillna(0)
# 
#                 # --- FIX: ROBUST TARGET MAPPING ---
#                 if 'Churn' in raw_df.columns:
#                     # 1. Convert to string, strip whitespace, make lowercase
#                     clean_churn = raw_df['Churn'].astype(str).str.strip().str.lower()
# 
#                     # 2. Map 'yes'->1, 'no'->0.
#                     # 3. Fill any failures (NaN) with 0 so the app doesn't crash
#                     st.session_state['test_targets'] = clean_churn.map({'yes': 1, 'no': 0}).fillna(0).values
# 
#                     features_df = raw_df.drop(columns=['Churn', 'customerID'], errors='ignore')
#                 else:
#                     st.session_state['test_targets'] = None
#                     features_df = raw_df.drop(columns=['customerID'], errors='ignore')
# 
#                 # Save features for display
#                 st.session_state['display_data'] = features_df.copy()
# 
#                 # PREPROCESSING
#                 preprocessor = load_assets()
#                 if preprocessor:
#                     st.session_state['test_data'] = preprocessor.transform(features_df)
#                     st.success(f"âœ… Loaded & Processed {len(raw_df)} customers.")
# 
#             except Exception as err:
#                 st.error(f"Error processing file: {err}")
# 
#     with right_col:
#         if st.session_state['test_data'] is not None:
#             st.subheader("2. Prediction Results")
#             active_model = get_trained_model(selected_algorithm)
# 
#             if active_model:
#                 X_in = st.session_state['test_data']
#                 y_true = st.session_state['test_targets']
# 
#                 try:
#                     # Generate Predictions
#                     pred_indices = active_model.predict(X_in)
#                     pred_names = [TARGET_CLASSES[i] for i in pred_indices]
# 
#                     # Display Data Table with Predictions
#                     display_df = st.session_state['display_data'].copy()
#                     display_df.insert(0, "âš ï¸ Risk Prediction", pred_names)
# 
#                     def highlight_churn(val):
#                         return 'background-color: #ffcccc' if val == 'Churn' else ''
# 
#                     st.dataframe(display_df.style.applymap(highlight_churn, subset=['âš ï¸ Risk Prediction']), height=300)
# 
#                     # --- METRICS SECTION ---
#                     if y_true is not None:
#                         st.markdown("### Performance Metrics")
#                         scores = compute_metrics(active_model, X_in, y_true)
# 
#                         if scores:
#                             # Metrics Row
#                             c1, c2, c3, c4, c5 = st.columns(5)
#                             c1.metric("Accuracy", f"{scores['Accuracy']:.1%}")
#                             c2.metric("AUC", f"{scores['AUC']:.3f}")
#                             c3.metric("F1-Score", f"{scores['F1']:.3f}")
#                             c4.metric("Precision", f"{scores['Precision']:.3f}")
#                             c5.metric("Recall", f"{scores['Recall']:.3f}")
# 
#                             # Confusion Matrix
#                             col_cm1, col_cm2 = st.columns([1, 2])
#                             with col_cm1:
#                                  st.write("#### Confusion Matrix")
#                                  fig_cm, ax_cm = plt.subplots(figsize=(4, 3))
#                                  cm_data = confusion_matrix(y_true, scores['predictions'])
#                                  sns.heatmap(
#                                     cm_data, annot=True, fmt='d', cmap='Reds',
#                                     xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes']
#                                  )
#                                  plt.xlabel('Predicted')
#                                  plt.ylabel('Actual')
#                                  st.pyplot(fig_cm)
#                         else:
#                             st.warning("âš ï¸ Could not calculate metrics. Target column contained errors.")
# 
#                 except Exception as e:
#                     st.error(f"Prediction failed: {e}")
#             else:
#                 st.error(f"Model file for '{selected_algorithm}' not found.")
# 
# # ---------------------------------------------------------
# # 6. PAGE: INSIGHTS
# # ---------------------------------------------------------
# else:
#     st.title("ðŸ“Š Model Benchmark & Insights")
#     CSV_PATH = 'model_performance.csv'
#     if os.path.exists(CSV_PATH):
#         df_results = pd.read_csv(CSV_PATH)
#         st.dataframe(df_results.sort_values(by="F1-Score", ascending=False), use_container_width=True)
#     else:
#         st.warning("âš ï¸ Benchmark file 'model_performance.csv' not found.")

# streamlit run app.py

import time

# 1. Kill any previous versions running in the background to free the port
!pkill -f streamlit

# 2. Install Streamlit (quietly)
!pip install -q streamlit

# 3. Run Streamlit in the BACKGROUND
# We send the output to 'streamlit.log' so we can check for errors if it fails again
print("â³ Starting Streamlit...")
!nohup streamlit run app.py > streamlit.log 2>&1 &

# 4. CRITICAL: Wait 5 seconds for it to fully boot up
time.sleep(5)

# 5. Get the IP Address (Password)
print("ðŸ‘‡ COPY THIS IP ADDRESS (You will need it for the password):")
!curl ipv4.icanhazip.com

# 6. Start the Tunnel
print("\nðŸš€ Click the link below (and paste the IP from above):")
!npx localtunnel --port 8501

import os
import time

# ---------------------------------------------------------
# 1. KILL ZOMBIE PROCESSES
# ---------------------------------------------------------
print("ðŸ”ª Killing old background processes...")
!pkill -f streamlit
!pkill -f cloudflared
!rm -f tunnel.log streamlit.log  # Delete old logs

# ---------------------------------------------------------
# 2. INSTALL & DOWNLOAD (If needed)
# ---------------------------------------------------------
if not os.path.exists("cloudflared-linux-amd64"):
    print("â¬‡ï¸ Downloading Cloudflare Tunnel...")
    !wget -q -nc https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
    !chmod +x cloudflared-linux-amd64

# ---------------------------------------------------------
# 3. START FRESH
# ---------------------------------------------------------
print("ðŸš€ Starting Streamlit...")
!nohup streamlit run app.py > streamlit.log 2>&1 &

print("â³ Waiting for app to load (5 seconds)...")
time.sleep(5)

print("ðŸ”— Starting Tunnel...")
!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:8501 > tunnel.log 2>&1 &

# ---------------------------------------------------------
# 4. GET THE LINK
# ---------------------------------------------------------
time.sleep(4)
print("\nðŸ‘‡ CLICK THIS LINK TO OPEN YOUR APP:")
!grep -o 'https://.*\.trycloudflare.com' tunnel.log | head -n 1

